{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 전체 feature에서 risk 제외로 mace 예측 / total data 대상 '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" 전체 feature에서 risk 제외로 mace 예측 / total data 대상 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122398, 27)\n",
      "['interval_statin', 'mace', 'group_after_detail_1', 'statin_drug', 'AGE', 'SEX', 'HTN', 'ICDHTN', 'trt_ICD_DM', 'DM', 'BMI', 'WAIST', 'BP_HIGH', 'BP_LWST', 'BLDS', 'TOT_CHOLE', 'TRIGLYCERIDE', 'HDL_CHOLE', 'LDL_CHOLE', 'HMG', 'smoking', 'econo', 'drinking', 'HMG.1', 'LDL_CHOLE.1', 'current_ascvd', 'risk']\n",
      "24\n",
      "['group_after_detail_1', 'statin_drug', 'AGE', 'SEX', 'HTN', 'ICDHTN', 'trt_ICD_DM', 'DM', 'BMI', 'WAIST', 'BP_HIGH', 'BP_LWST', 'BLDS', 'TOT_CHOLE', 'TRIGLYCERIDE', 'HDL_CHOLE', 'LDL_CHOLE', 'HMG', 'smoking', 'econo', 'drinking', 'HMG.1', 'LDL_CHOLE.1', 'current_ascvd']\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch import tensor \n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "import itertools\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "total_out = Path(os.getcwd()+'/Statin_preprocess.csv')\n",
    "total_data= pd.read_csv(total_out)\n",
    "print(total_data.shape)\n",
    "print(list(total_data.columns))\n",
    "\n",
    "target ='mace'\n",
    "\n",
    "# mri feature가 시작하는 column의 index 구하기\n",
    "# np.where의 결과값이 array에 들어가기 때문에 방금 계산해 넣어놓은 [0]번째 값을 가져온다.\n",
    "start_features_index = np.where(total_data.columns.values == \"group_after_detail_1\")[0][0]\n",
    "features = list(total_data.columns[start_features_index:])\n",
    "features.remove('risk')\n",
    "\n",
    "print(len(features))\n",
    "print(features)\n",
    "\n",
    "Num_feat = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(Num_feat, clf, test_data_processed, features):\n",
    "    importance =clf.feature_importances_\n",
    "    #plt.plot(importance)\n",
    "    #plt.show()\n",
    "    labels_importance=importance.argsort()[::-1]\n",
    "\n",
    "    importance_sort = np.sort(importance)[::-1]\n",
    "\n",
    "    feat_name_sort=test_data_processed[features].columns[labels_importance]\n",
    "    important_features = pd.DataFrame() \n",
    "    \n",
    "    for i in range (Num_feat):\n",
    "        feature = pd.DataFrame([[feat_name_sort[i],importance_sort[i]]], columns = ['feature name', 'ratio'])\n",
    "        important_features=pd.concat([important_features,feature])\n",
    "\n",
    "    return important_features.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented\n",
    "import torch\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def find_bestpar(train_data, test_data, features):\n",
    "    \n",
    "    valid_cut = (int)(len(train_data)*0.8)\n",
    "    for_Train = train_data[:valid_cut]\n",
    "    for_Validation = train_data[valid_cut:]\n",
    "    \n",
    "    \"\"\"train data 생성\"\"\"\n",
    "    X_train = for_Train[features].values\n",
    "    Y_train = for_Train[target].values\n",
    "    \n",
    "    \"\"\"valid data 생성\"\"\"\n",
    "    X_valid = for_Validation[features].values\n",
    "    Y_valid = for_Validation[target].values\n",
    "    \n",
    "    \"\"\"test data 생성\"\"\"\n",
    "    X_test = test_data[features].values\n",
    "    Y_test = test_data[target].values\n",
    "    \n",
    "    print(len(X_train), len(X_valid), len(X_test))            \n",
    "    \n",
    "    # Store maximum auc\n",
    "    max_auc= 0\n",
    "    # Store maximum hypterparameter set\n",
    "    max_hy = []\n",
    "    \n",
    "    \"\"\"\n",
    "    # define hyperparameter space : learning rate, \n",
    "    n_ = [4,8,16]                              # \n",
    "    lr_ = [2e-2, 1e-2, 5e-3, 2e-3, 1e-3, 1e-4] # learning rate\n",
    "    w_ = [0.01, 0.001, 0.0001]                 # weight decay\n",
    "    g_ = [0.95, 0.99, 0.9]                     # scheduler params - gamma\n",
    "    ss_ = [10, 20, 30]                         # scheduler params - step_size\n",
    "    \"\"\"\n",
    "    # Orginal hyperparameter space \n",
    "    \n",
    "    # define hyperparameter space (quick version)\n",
    "    n_ = [4,16]\n",
    "    lr_ = [2e-2,1e-3]\n",
    "    w_ = [0.01,0.001]\n",
    "    g_ = [0.95,0.99]\n",
    "    ss_ = [10,30]\n",
    "    \n",
    "    all_ = [n_, lr_, w_, g_, ss_]\n",
    "    h_space = [s for s in itertools.product(*all_)]\n",
    "    \n",
    "    print(\"start training\")\n",
    "    count=0\n",
    "    for hy in tqdm(h_space):\n",
    "        count = count + 1\n",
    "        clf = TabNetClassifier(n_a = hy[0],\n",
    "                                n_d = hy[0],\n",
    "                                optimizer_params = dict(lr=hy[1], weight_decay=hy[2]),\n",
    "                                scheduler_params={\"step_size\":hy[4], \"gamma\":hy[3]},\n",
    "                                scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                                verbose=0)\n",
    "\n",
    "        clf.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "                    eval_name=['train', 'valid'], eval_metric=['auc'],\n",
    "                    max_epochs=200 , patience=20)\n",
    "       \n",
    "        preds_acc = clf.predict(X_test)\n",
    "        preds_prob = clf.predict_proba(X_test)\n",
    "        test_auc = roc_auc_score(y_score=preds_prob[:,1], y_true=Y_test)\n",
    "        test_acc = accuracy_score(preds_acc, Y_test)\n",
    "            \n",
    "        print('Valid score: %2f'% clf.best_cost, 'Test AUC: %.3f%%'%test_auc, 'Test ACC: %.3f%%'%test_acc)\n",
    "    \n",
    "        if np.mean(test_auc)>max_auc:\n",
    "            print(\"Find new maximum test AUC!!\\n\")\n",
    "            max_hy = hy\n",
    "            max_valid_score = clf.best_cost\n",
    "            max_auc = test_auc\n",
    "            max_acc = test_acc\n",
    "    \n",
    "    return clf, max_hy, max_valid_score, max_auc, max_acc, preds_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preprocessing\n",
      "122398\n"
     ]
    }
   ],
   "source": [
    "def preprocessing (total_data):\n",
    "    total_data_processed= total_data.fillna(0).reset_index(drop=True)\n",
    "    \n",
    "    # frac: 전체 row 중 몇 %를 반환할 지 결정 -> frac=1을 설정해서 모든 데이터를 반환\n",
    "    # random_state: 추후 이것과 동일한 샘플링을 재현하기 위함\n",
    "    # sample: 데이터에서 임의의 샘플 선정 -> frac=1이면 전체 data의 순서만 임의로 바뀜\n",
    "    total_data_processed = total_data_processed.sample(frac=1,random_state=2020).reset_index(drop=True)\n",
    "    \n",
    "    print(\"done preprocessing\")\n",
    "    return total_data_processed\n",
    "\n",
    "total_data = preprocessing(total_data)\n",
    "print(len(total_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cut = (int)(len(total_data)*0.8)\n",
    "#print(train_cut)                 \n",
    "\n",
    "train_data = total_data[:train_cut] \n",
    "test_data = total_data[train_cut:] \n",
    "#print(len(train_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, train_data, test_data, Num_feat, features):\n",
    "        clf, max_hy, max_valid_score, max_test_auc, max_test_acc, preds_prob = find_bestpar(train_data, test_data, features)    \n",
    "    \n",
    "        self.train_data = train_data\n",
    "        self.test_data = test_data\n",
    "        self.preds_prob = preds_prob \n",
    "        self.max_hy = max_hy\n",
    "        self.test_auc = max_test_auc\n",
    "        self.test_acc = max_test_acc\n",
    "        self.valid_score = max_valid_score\n",
    "        self.clf = clf\n",
    "        self.features = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78334 19584 24480\n",
      "start training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "353de6ec06eb40adbd911f699e61cd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_valid_auc = 0.7138\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.713805 Test AUC: 0.718% Test ACC: 0.960%\n",
      "Find new maximum test AUC!!\n",
      "\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 16 and best_valid_auc = 0.71364\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.713641 Test AUC: 0.726% Test ACC: 0.960%\n",
      "Find new maximum test AUC!!\n",
      "\n",
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 6 and best_valid_auc = 0.71079\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.710795 Test AUC: 0.715% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 16 and best_valid_auc = 0.71364\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.713641 Test AUC: 0.726% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 21 and best_valid_auc = 0.72246\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722463 Test AUC: 0.727% Test ACC: 0.960%\n",
      "Find new maximum test AUC!!\n",
      "\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_valid_auc = 0.72244\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722437 Test AUC: 0.721% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 57 with best_epoch = 37 and best_valid_auc = 0.7213\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.721302 Test AUC: 0.727% Test ACC: 0.960%\n",
      "Find new maximum test AUC!!\n",
      "\n",
      "\n",
      "Early stopping occurred at epoch 44 with best_epoch = 24 and best_valid_auc = 0.72244\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722437 Test AUC: 0.721% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 89 and best_valid_auc = 0.72357\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.723569 Test AUC: 0.730% Test ACC: 0.960%\n",
      "Find new maximum test AUC!!\n",
      "\n",
      "\n",
      "Early stopping occurred at epoch 99 with best_epoch = 79 and best_valid_auc = 0.72285\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722850 Test AUC: 0.727% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 77 with best_epoch = 57 and best_valid_auc = 0.72246\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722462 Test AUC: 0.729% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 89 and best_valid_auc = 0.72257\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722572 Test AUC: 0.728% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 128 with best_epoch = 108 and best_valid_auc = 0.71303\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.713031 Test AUC: 0.723% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 160 with best_epoch = 140 and best_valid_auc = 0.72176\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.721760 Test AUC: 0.721% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 73 and best_valid_auc = 0.72016\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.720157 Test AUC: 0.710% Test ACC: 0.960%\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 194 and best_valid_auc = 0.72292\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722919 Test AUC: 0.714% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_valid_auc = 0.71134\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.711340 Test AUC: 0.718% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_valid_auc = 0.71134\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.711340 Test AUC: 0.718% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_valid_auc = 0.71134\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.711340 Test AUC: 0.718% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 1 and best_valid_auc = 0.71134\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.711340 Test AUC: 0.718% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.72291\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722909 Test AUC: 0.721% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.72291\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722909 Test AUC: 0.721% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.72291\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722909 Test AUC: 0.721% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 8 and best_valid_auc = 0.72291\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.722909 Test AUC: 0.721% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_valid_auc = 0.72544\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.725443 Test AUC: 0.730% Test ACC: 0.960%\n",
      "Find new maximum test AUC!!\n",
      "\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 46 and best_valid_auc = 0.72547\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.725472 Test AUC: 0.731% Test ACC: 0.960%\n",
      "Find new maximum test AUC!!\n",
      "\n",
      "\n",
      "Early stopping occurred at epoch 54 with best_epoch = 34 and best_valid_auc = 0.72505\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.725049 Test AUC: 0.732% Test ACC: 0.960%\n",
      "Find new maximum test AUC!!\n",
      "\n",
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 50 and best_valid_auc = 0.72534\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.725344 Test AUC: 0.731% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 95 with best_epoch = 75 and best_valid_auc = 0.7122\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.712202 Test AUC: 0.721% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 85 with best_epoch = 65 and best_valid_auc = 0.71556\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.715562 Test AUC: 0.711% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 137 with best_epoch = 117 and best_valid_auc = 0.71875\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.718750 Test AUC: 0.711% Test ACC: 0.960%\n",
      "\n",
      "Early stopping occurred at epoch 56 with best_epoch = 36 and best_valid_auc = 0.70941\n",
      "Best weights from best epoch are automatically used!\n",
      "Valid score: 0.709407 Test AUC: 0.709% Test ACC: 0.960%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TabNet_mace_without_risk_model = model(train_data, test_data, Num_feat, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<Important Feature>>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature name</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group_after_detail_1</td>\n",
       "      <td>0.101471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGE</td>\n",
       "      <td>0.100735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DM</td>\n",
       "      <td>0.095832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HTN</td>\n",
       "      <td>0.070162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMI</td>\n",
       "      <td>0.067715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ICDHTN</td>\n",
       "      <td>0.065037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>statin_drug</td>\n",
       "      <td>0.059377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>smoking</td>\n",
       "      <td>0.045407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trt_ICD_DM</td>\n",
       "      <td>0.038795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BLDS</td>\n",
       "      <td>0.034668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HMG</td>\n",
       "      <td>0.034454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>current_ascvd</td>\n",
       "      <td>0.034256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WAIST</td>\n",
       "      <td>0.033681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HMG.1</td>\n",
       "      <td>0.026481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>econo</td>\n",
       "      <td>0.025548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>SEX</td>\n",
       "      <td>0.024266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LDL_CHOLE</td>\n",
       "      <td>0.021337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BP_LWST</td>\n",
       "      <td>0.020417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>drinking</td>\n",
       "      <td>0.019077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BP_HIGH</td>\n",
       "      <td>0.018373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>TRIGLYCERIDE</td>\n",
       "      <td>0.016952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LDL_CHOLE.1</td>\n",
       "      <td>0.016622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>HDL_CHOLE</td>\n",
       "      <td>0.016354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TOT_CHOLE</td>\n",
       "      <td>0.012982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature name     ratio\n",
       "0   group_after_detail_1  0.101471\n",
       "1                    AGE  0.100735\n",
       "2                     DM  0.095832\n",
       "3                    HTN  0.070162\n",
       "4                    BMI  0.067715\n",
       "5                 ICDHTN  0.065037\n",
       "6            statin_drug  0.059377\n",
       "7                smoking  0.045407\n",
       "8             trt_ICD_DM  0.038795\n",
       "9                   BLDS  0.034668\n",
       "10                   HMG  0.034454\n",
       "11         current_ascvd  0.034256\n",
       "12                 WAIST  0.033681\n",
       "13                 HMG.1  0.026481\n",
       "14                 econo  0.025548\n",
       "15                   SEX  0.024266\n",
       "16             LDL_CHOLE  0.021337\n",
       "17               BP_LWST  0.020417\n",
       "18              drinking  0.019077\n",
       "19               BP_HIGH  0.018373\n",
       "20          TRIGLYCERIDE  0.016952\n",
       "21           LDL_CHOLE.1  0.016622\n",
       "22             HDL_CHOLE  0.016354\n",
       "23             TOT_CHOLE  0.012982"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"<<Important Feature>>\")\n",
    "import_feat=feature(Num_feat, TabNet_mace_without_risk_model.clf, TabNet_mace_without_risk_model.test_data, TabNet_mace_without_risk_model.features)\n",
    "TabNet_mace_without_risk_model.import_feat =  import_feat\n",
    "import_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('./TabNet_mace_without_risk_model.pkl', 'wb') as f:\n",
    "    dill.dump(TabNet_mace_without_risk_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9587826797385621\n",
      "0.6862693183334976\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\"\"\"\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\"\"\"\n",
    "\n",
    "train_cut = (int)(len(total_data)*0.8)\n",
    "#print(train_cut)         \n",
    "\n",
    "train_data = total_data[:train_cut] \n",
    "test_data = total_data[train_cut:] \n",
    "\n",
    "X_train = train_data[features].values\n",
    "y_train = train_data[target].values\n",
    "\n",
    "X_test = test_data[features].values\n",
    "y_test = test_data[target].values\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42, eval_metric=\"auc\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "y_pred_prob = xgb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, y_pred_prob))\n",
    "#print(xgb_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
